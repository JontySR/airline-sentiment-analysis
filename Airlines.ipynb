{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Airlines.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN7kywj/KdOn4X5ZmBZPA6X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JontySR/airline-sentiment-analysis/blob/master/Airlines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfGczegFOVXN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "8a86cdbb-822e-4792-b9b4-908b951352b3"
      },
      "source": [
        "import spacy\n",
        "import re\n",
        "import csv\n",
        "from spacy.lang.en import English\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "#from spacy import Lemmatizer\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "nlp = spacy.load('en')#\n",
        "dataset = pd.read_csv('/content/Tweets.csv')\n",
        "\n",
        "text = dataset['text']\n",
        "sentiment = dataset['airline_sentiment']\n",
        "confidence = dataset['airline_sentiment_confidence']\n",
        "airlines = dataset['airline']\n",
        "#GET TWEETS WHICH ARE POSITIVE\n",
        "def get_most_common_words(word, dataset):\n",
        " \n",
        "  collection = ''\n",
        "  i = 0\n",
        "  while i < len(text) -1000:\n",
        "    if sentiment[i] == word and confidence[i] > 0.5:\n",
        "      collection = ''.join([collection, text[i], ' '])\n",
        "    i+=1\n",
        "  with open('fulltext.txt', 'w') as output:\n",
        "    output.write(collection)\n",
        "  doc = nlp(collection)\n",
        "  nouns = [token.text.upper() for token in doc if token.pos_ == 'NOUN' and token.text !='#']\n",
        "  word_freq = Counter(nouns)\n",
        "  return word_freq.most_common(30)\n",
        "set1 = get_most_common_words('positive',dataset)\n",
        "set2 = get_most_common_words('negative',dataset)\n",
        "set3 = get_most_common_words('neutral',dataset)\n",
        "word_pos = [element[:][0] for element in set1]\n",
        "word_neu = [element[:][0] for element in set2]\n",
        "word_neg = [element[:][0] for element in set3]\n",
        "unique_pos = [element for element in word_pos if element not in word_neu and element not in word_neg]\n",
        "unique_neu = [element for element in word_neu if element not in word_pos and element not in word_neg]\n",
        "unique_neg = [element for element in word_neg if element not in word_pos and element not in word_neu]\n",
        "common_to_all = [element for element in word_pos if element in word_neu and element in word_neg]\n",
        "print(unique_pos)\n",
        "print(unique_neu)\n",
        "print(unique_neg)\n",
        "print(common_to_all)\n",
        "############################################################################################################\n",
        "from textblob import TextBlob\n",
        "from textblob.sentiments import PatternAnalyzer\n",
        "\n",
        "virgin_america = [0,0]\n",
        "united = [0,0]\n",
        "southwest = [0,0]\n",
        "delta = [0,0]\n",
        "us_airways = [0,0]\n",
        "american = [0,0]\n",
        "index = 0\n",
        "def increment(list, index):\n",
        "  list[0] += TextBlob(text[index],analyzer=PatternAnalyzer()).sentiment[1]\n",
        "  list[1] +=1\n",
        "while index < len(airlines):\n",
        "  if airlines[index] == 'Virgin America':\n",
        "    increment(virgin_america, index)\n",
        "  elif airlines[index] == 'United':\n",
        "    increment(united, index)\n",
        "  elif airlines[index] == 'Southwest':\n",
        "    increment(southwest, index)\n",
        "  elif airlines[index] == 'Delta':\n",
        "    increment(delta, index)\n",
        "  elif airlines[index] == 'US Airways':\n",
        "    increment(us_airways, index)\n",
        "  elif airlines[index] == 'American':\n",
        "    increment(american, index)\n",
        "  index+=1\n",
        "print('VIRGIN_AMERICA: ' + str(virgin_america[0]/virgin_america[1]))\n",
        "print(virgin_america[1])\n",
        "print('UNITED: ' + str(united[0]/united[1]))\n",
        "print(united[1])\n",
        "print('SOUTHWEST: ' + str(southwest[0]/southwest[1]))\n",
        "print(southwest[1])\n",
        "print('DELTA: ' + str(delta[0]/delta[1]))\n",
        "print(delta[1])\n",
        "print('US_AIRWAYS: ' + str(us_airways[0]/us_airways[1]))\n",
        "print(us_airways[1])\n",
        "print('AMERICAN: ' + str(american[0]/american[1]))\n",
        "print(american[1])\n",
        "print(virgin_america[1] + united[1] + southwest[1] + delta[1] + us_airways[1] + american[1])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['CREW', 'RESPONSE', 'STAFF', ':)', 'JOB', '🙏', 'LOVE', 'TEAM', 'WORK', 'TONIGHT', 'EXPERIENCE']\n",
            "['HOURS', 'HOLD', 'HOUR', 'MINUTES', 'PEOPLE', 'BAGS', 'DELAY', 'LUGGAGE', 'DAYS', '@USAIRWAYS']\n",
            "['FLEEK', 'FLEET', 'TOMORROW', 'NUMBER', 'EMAIL', 'TICKET', 'TRAVEL', 'TICKETS', 'PASSENGERS', 'RESERVATION', 'CHANCE', 'NAME', 'MILES']\n",
            "['THANKS', 'FLIGHT', 'SERVICE', 'GUYS', 'TIME', 'TODAY', 'HELP', 'FLIGHTS', 'PLANE', 'WAY', 'AIRPORT', 'WEATHER']\n",
            "VIRGIN_AMERICA: 0.367723843570818\n",
            "504\n",
            "UNITED: 0.36338957118719073\n",
            "3822\n",
            "SOUTHWEST: 0.364339172846973\n",
            "2420\n",
            "DELTA: 0.35559377023907895\n",
            "2222\n",
            "US_AIRWAYS: 0.37101308549081513\n",
            "2913\n",
            "AMERICAN: 0.34710677853954436\n",
            "2759\n",
            "14640\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9UUKbsfAKbX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "def split_csv(file, keyword, reader):\n",
        "  with open(file, 'w') as new_file:\n",
        "    writer = csv.writer(new_file,delimiter=',')\n",
        "    for line in reader:\n",
        "      if keyword in line:\n",
        "        writer.writerow(line)\n",
        "with open('/content/Tweets.csv','r') as csv_file:\n",
        "  reader = csv.reader(csv_file)\n",
        "  #split_csv('/content/united.csv','United', reader)\n",
        "  split_csv('/content/virgin.csv','Virgin America', reader)\n",
        "  split_csv('/content/american.csv','American', reader)\n",
        "  split_csv('/content/southwest.csv','Southwest', reader)\n",
        "  split_csv('/content/delta.csv','Delta', reader)\n",
        "  split_csv('/content/us_airways.csv','US Airways', reader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnnKcb-oNiCA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "with open('/content/Tweets.csv','r') as csv_file:\n",
        "  reader = csv.reader(csv_file)\n",
        "  with open('united.csv','w') as new_file:\n",
        "    writer = csv.writer(new_file,delimiter=',')\n",
        "    for line in reader:\n",
        "      if 'United' in line:\n",
        "\n",
        "        writer.writerow(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fOLGTppxfz4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "fa08a808-d348-4a21-e417-47804c2c9811"
      },
      "source": [
        "import nltk\n",
        "\n",
        "from nltk.corpus import webtext\n",
        "from nltk.collocations import BigramCollocationFinder\n",
        "from nltk.collocations import BigramAssocMeasures\n",
        "from nltk.corpus import stopwords\n",
        "stopset = set(stopwords.words('english'))\n",
        "with open('/content/fulltext.txt','r') as full_text:\n",
        "  text = full_text.read()\n",
        "  with open('/content/stoptext.txt','w') as stop_text:\n",
        "    for word in text:\n",
        "      if word not in stopset:\n",
        "\n",
        "words = [w.upper() for w in webtext.words('/content/fulltext.txt')]\n",
        "finder = BigramCollocationFinder.from_words(words)\n",
        "print(finder.nbest(BigramAssocMeasures.likelihood_ratio,100))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-22-57b982a29e2c>\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    words = [w.upper() for w in webtext.words('/content/fulltext.txt')]\u001b[0m\n\u001b[0m                                                                       ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tI84ckeO7TmW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "ff33cc84-e0f2-46e0-8f28-8b765d844f29"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en')\n",
        "with open('/content/fulltext.txt','r') as full_text:\n",
        "  text = full_text.read()\n",
        "ner_text = nlp(text)\n",
        "entities = []\n",
        "\n",
        "for word in ner_text.ents:\n",
        "  if word.text.upper() not in entities and word.label_ == 'GPE':\n",
        "    entities.append(word.text.upper())\n",
        "print(entities)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['@VIRGINAMERICA', 'HAWAII', 'DALLAS', 'SEATTLE', 'ATLANTA', 'LA', 'SF', 'LGA', 'EMIRATES', 'AUSTIN', 'VEGAS', 'DC', 'NEW YORK', 'SANTA CRUZ', 'LAS', 'PARIS', 'BOSTON', 'VIRGIN MOBILE MEXICO', 'PHILADELPHIA', 'TEXAS', 'SAN JOSE', 'NEW YORK HTTP://T.CO/HAQC7GDG7C', 'MINNEAPOLIS', 'GLASGOW', 'U.S.', 'NEWARK', 'MIAMI', 'DMED', 'AUSTRALIA', 'DENVER', 'KC', 'CHICAGO', 'CHINA', 'MALAYSIA', 'KUALA LUMPUR', 'NARAYANAN', 'MANCHESTER', 'ETHIOPIA', 'HOUSTON', 'RENO', 'YVR', 'MEXICO', 'US', 'LAX', 'VENEZUELA', 'FRANKFURT', 'MELBOURNE', 'BOGOTA', 'COLOMBIA', 'ALBUQUERQUE', 'NM', 'USA', 'CEBU', 'PHILIPPINES', 'ASPEN', 'UAL212', 'HAYDEN', 'UA', \"O'HARE\", 'LABRADOR', 'CANADA', 'SAN JUAN', 'SACRAMENTO', 'DETROIT', 'FLORIDA', 'SAN DIEGO', 'MEMPHIS', 'LAS VEGAS', 'CALGARY', 'BANGKOK', 'IND', 'NASHVILLE', 'JJ', 'THE UNITED APP', 'BELFAST', 'PERTH', 'PORTLAND', 'SCHIPHOL', 'SANTA BARBARA', 'NETHERLANDS', \"ST. JOHN'S\", 'ANTIGUA', 'FL', 'IL', 'TOKYO', 'LONDON', 'SYDNEY', 'ASIA PAC', 'FORT WORTH', '@FLYTPA', 'SEA', 'ROC', 'INDIA', 'AMSTERDAM', 'EWR', 'PALM SPRINGS', 'HEMISPHERE', 'DURANGO', 'FARMINGTON NEW MEXICO', 'VIRGIN', 'GLASSDOOR', 'JACKSONVILLE', 'MIDWAY-CHICAGO', 'BIRMINGHAM', 'CHANTILLY', 'FRANCE', 'TAMPA', 'WASHINGTON DC', 'AMERICA', 'MIDLAND', 'L.A.', 'SAN ANTONIO', 'OHIO', 'OAKLAND', 'SAN FRAN', 'LASALLE', 'BOS', 'NEW ORLEANS', 'ARUBA', 'SAN FRANSISCO', 'ST. LOUIS', 'ARKANSAS', 'LGBT.CAN', '3130', 'AZ', 'OMAHA', 'TAIWAN', 'BALTIMORE', 'INDIANAPOLIS', 'N231WN', 'COLUMBUS', 'FAIRFAX', 'MEXICO CITY', 'MIDWAY', 'BOISE', 'UTAH', 'FORT LAUDERDALE', 'COLD', 'PROVO', 'PLEASEEEEEE', 'HOU', 'PUERTO VALLARTA', '@BOSTONLOGAN', 'MILWAUKEE', 'ORLANDO', 'BEIJING', 'SAVANNAH', 'IRELAND', 'PHOENIX', '@VINYLVEGAS', 'SAN DIEGO @SOUTHWESTAIR', 'DOMINICAN REPUBLIC', 'JKF', 'HTTP://T.CO/VXN2J36M7V', 'CHARLESTON', 'HTTP://T.CO/KWUEK1UKBC', 'BAHAMAS', 'HTTP://T.CO/IRIXAIFJJX', 'YASSSSS', 'WELLLLLLL', '😕', 'HTTP://T.CO/PULP4I0W96', 'CUBA', 'PITTSBURGH', '☺', 'CPAP', 'NASSAU', 'PITTS', 'HAITI', 'CALIFORNIA', 'ST LUCIA', 'BQN', 'D.C.', 'HAVANA', 'CARTAGO', 'CLEVELAND', 'HUNTSVILLE', 'AL', 'BRAZIL', 'ICELAND', 'ITALY', 'NAPLES', 'NEW LIVERY', '@BOSTONLOGAN HTTP://T.CO/QEADA92MW6 @JETBLUE', 'WASHINGTON  DC', 'PUERTO RICO', 'JAMAICA', 'WASHINGTON', 'CHARLOTTE', 'CAROLINAS', 'ISRAEL', 'UK', 'QUINTANA ROO', '@PDQUIGLEY', 'DFW', 'KPHL', 'LYNCHBURG', 'ONTARIO', 'LEXINGTON', 'GAINESVILLE', 'USAIRWAYS', 'ARIZONA', 'NC', 'SAN MIGUEL DE ALLENDE', 'BARBADOS', 'AM.SHOULD', 'NUTELLA', 'INTRA-EU', 'WEST PALM BEACH', 'DUBLIN', 'HERNDON', 'VIRGINIA', 'AUH', 'ECUADOR', 'RUSSIA', 'NO', 'OREGON', 'ALASKA', 'ZURICH', 'SAN DIEGO @AMERICANAIR']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6sO71qRJKi-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#next thing we're bringing in is the United Airlines corpus. \n",
        "#If a tweet is negative, append its text to the rough text file and then do NLPre on it\n",
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "dataset = pd.read_csv('united.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}