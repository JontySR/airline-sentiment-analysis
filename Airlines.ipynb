{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Airlines.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfGczegFOVXN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "ab89568c-30cb-4f4a-dd07-780f1650b9d7"
      },
      "source": [
        "import spacy\n",
        "import re\n",
        "import csv\n",
        "from spacy.lang.en import English\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "#from spacy import Lemmatizer\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "nlp = spacy.load('en')#\n",
        "dataset = pd.read_csv('/content/Tweets.csv')\n",
        "\n",
        "text = dataset['text']\n",
        "sentiment = dataset['airline_sentiment']\n",
        "confidence = dataset['airline_sentiment_confidence']\n",
        "airlines = dataset['airline']\n",
        "#GET TWEETS WHICH ARE POSITIVE\n",
        "def get_most_common_words(word, dataset):\n",
        " \n",
        "  collection = ''\n",
        "  i = 0\n",
        "  while i < len(text) -1000:\n",
        "    if sentiment[i] == word and confidence[i] > 0.5:\n",
        "      collection = ''.join([collection, text[i], ' '])\n",
        "    i+=1\n",
        "  with open('fulltext.txt', 'w') as output:\n",
        "    output.write(collection)\n",
        "  doc = nlp(collection)\n",
        "  nouns = [token.text.upper() for token in doc if token.pos_ == 'NOUN' and token.text !='#']\n",
        "  word_freq = Counter(nouns)\n",
        "  return word_freq.most_common(30)\n",
        "set1 = get_most_common_words('positive',dataset)\n",
        "set2 = get_most_common_words('negative',dataset)\n",
        "set3 = get_most_common_words('neutral',dataset)\n",
        "word_pos = [element[:][0] for element in set1]\n",
        "word_neu = [element[:][0] for element in set2]\n",
        "word_neg = [element[:][0] for element in set3]\n",
        "unique_pos = [element for element in word_pos if element not in word_neu and element not in word_neg]\n",
        "unique_neu = [element for element in word_neu if element not in word_pos and element not in word_neg]\n",
        "unique_neg = [element for element in word_neg if element not in word_pos and element not in word_neu]\n",
        "common_to_all = [element for element in word_pos if element in word_neu and element in word_neg]\n",
        "with open('fulltext.txt','r') as collection:\n",
        "  hashtags = {tag.strip(\"#\") for tag in collection.split if tag.startswith(\"#\")}\n",
        "  print(hashtags)\n",
        "print(unique_pos)\n",
        "print(unique_neu)\n",
        "print(unique_neg)\n",
        "print(common_to_all)\n",
        "############################################################################################################\n",
        "from textblob import TextBlob\n",
        "from textblob.sentiments import PatternAnalyzer\n",
        "\n",
        "virgin_america = [0,0]\n",
        "united = [0,0]\n",
        "southwest = [0,0]\n",
        "delta = [0,0]\n",
        "us_airways = [0,0]\n",
        "american = [0,0]\n",
        "index = 0\n",
        "def increment(list, index):\n",
        "  list[0] += TextBlob(text[index],analyzer=PatternAnalyzer()).sentiment[1]\n",
        "  list[1] +=1\n",
        "while index < len(airlines):\n",
        "  if airlines[index] == 'Virgin America':\n",
        "    increment(virgin_america, index)\n",
        "  elif airlines[index] == 'United':\n",
        "    increment(united, index)\n",
        "  elif airlines[index] == 'Southwest':\n",
        "    increment(southwest, index)\n",
        "  elif airlines[index] == 'Delta':\n",
        "    increment(delta, index)\n",
        "  elif airlines[index] == 'US Airways':\n",
        "    increment(us_airways, index)\n",
        "  elif airlines[index] == 'American':\n",
        "    increment(american, index)\n",
        "  index+=1\n",
        "print('VIRGIN_AMERICA: ' + str(virgin_america[0]/virgin_america[1]))\n",
        "print(virgin_america[1])\n",
        "print('UNITED: ' + str(united[0]/united[1]))\n",
        "print(united[1])\n",
        "print('SOUTHWEST: ' + str(southwest[0]/southwest[1]))\n",
        "print(southwest[1])\n",
        "print('DELTA: ' + str(delta[0]/delta[1]))\n",
        "print(delta[1])\n",
        "print('US_AIRWAYS: ' + str(us_airways[0]/us_airways[1]))\n",
        "print(us_airways[1])\n",
        "print('AMERICAN: ' + str(american[0]/american[1]))\n",
        "print(american[1])\n",
        "print(virgin_america[1] + united[1] + southwest[1] + delta[1] + us_airways[1] + american[1])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-4359eb6279aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0munique_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0melement\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_neg\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_pos\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_neu\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mcommon_to_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0melement\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_pos\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_neu\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_neg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mhashtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"#\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"#\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhashtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'collection' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9UUKbsfAKbX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "def split_csv(file, keyword, reader):\n",
        "  with open(file, 'w') as new_file:\n",
        "    writer = csv.writer(new_file,delimiter=',')\n",
        "    for line in reader:\n",
        "      if keyword in line:\n",
        "        writer.writerow(line)\n",
        "with open('/content/Tweets.csv','r') as csv_file:\n",
        "  reader = csv.reader(csv_file)\n",
        "  #split_csv('/content/united.csv','United', reader)\n",
        "  split_csv('/content/virgin.csv','Virgin America', reader)\n",
        "  split_csv('/content/american.csv','American', reader)\n",
        "  split_csv('/content/southwest.csv','Southwest', reader)\n",
        "  split_csv('/content/delta.csv','Delta', reader)\n",
        "  split_csv('/content/us_airways.csv','US Airways', reader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnnKcb-oNiCA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "with open('/content/Tweets.csv','r') as csv_file:\n",
        "  reader = csv.reader(csv_file)\n",
        "  with open('united.csv','w') as new_file:\n",
        "    writer = csv.writer(new_file,delimiter=',')\n",
        "    for line in reader:\n",
        "      if 'United' in line:\n",
        "\n",
        "        writer.writerow(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fOLGTppxfz4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "fa08a808-d348-4a21-e417-47804c2c9811"
      },
      "source": [
        "import nltk\n",
        "\n",
        "from nltk.corpus import webtext\n",
        "from nltk.collocations import BigramCollocationFinder\n",
        "from nltk.collocations import BigramAssocMeasures\n",
        "from nltk.corpus import stopwords\n",
        "stopset = set(stopwords.words('english'))\n",
        "with open('/content/fulltext.txt','r') as full_text:\n",
        "  text = full_text.read()\n",
        "  with open('/content/stoptext.txt','w') as stop_text:\n",
        "    for word in text:\n",
        "      if word not in stopset:\n",
        "\n",
        "words = [w.upper() for w in webtext.words('/content/fulltext.txt')]\n",
        "finder = BigramCollocationFinder.from_words(words)\n",
        "print(finder.nbest(BigramAssocMeasures.likelihood_ratio,100))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-22-57b982a29e2c>\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    words = [w.upper() for w in webtext.words('/content/fulltext.txt')]\u001b[0m\n\u001b[0m                                                                       ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tI84ckeO7TmW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "ff33cc84-e0f2-46e0-8f28-8b765d844f29"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en')\n",
        "with open('/content/fulltext.txt','r') as full_text:\n",
        "  text = full_text.read()\n",
        "ner_text = nlp(text)\n",
        "entities = []\n",
        "\n",
        "for word in ner_text.ents:\n",
        "  if word.text.upper() not in entities and word.label_ == 'GPE':\n",
        "    entities.append(word.text.upper())\n",
        "print(entities)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['@VIRGINAMERICA', 'HAWAII', 'DALLAS', 'SEATTLE', 'ATLANTA', 'LA', 'SF', 'LGA', 'EMIRATES', 'AUSTIN', 'VEGAS', 'DC', 'NEW YORK', 'SANTA CRUZ', 'LAS', 'PARIS', 'BOSTON', 'VIRGIN MOBILE MEXICO', 'PHILADELPHIA', 'TEXAS', 'SAN JOSE', 'NEW YORK HTTP://T.CO/HAQC7GDG7C', 'MINNEAPOLIS', 'GLASGOW', 'U.S.', 'NEWARK', 'MIAMI', 'DMED', 'AUSTRALIA', 'DENVER', 'KC', 'CHICAGO', 'CHINA', 'MALAYSIA', 'KUALA LUMPUR', 'NARAYANAN', 'MANCHESTER', 'ETHIOPIA', 'HOUSTON', 'RENO', 'YVR', 'MEXICO', 'US', 'LAX', 'VENEZUELA', 'FRANKFURT', 'MELBOURNE', 'BOGOTA', 'COLOMBIA', 'ALBUQUERQUE', 'NM', 'USA', 'CEBU', 'PHILIPPINES', 'ASPEN', 'UAL212', 'HAYDEN', 'UA', \"O'HARE\", 'LABRADOR', 'CANADA', 'SAN JUAN', 'SACRAMENTO', 'DETROIT', 'FLORIDA', 'SAN DIEGO', 'MEMPHIS', 'LAS VEGAS', 'CALGARY', 'BANGKOK', 'IND', 'NASHVILLE', 'JJ', 'THE UNITED APP', 'BELFAST', 'PERTH', 'PORTLAND', 'SCHIPHOL', 'SANTA BARBARA', 'NETHERLANDS', \"ST. JOHN'S\", 'ANTIGUA', 'FL', 'IL', 'TOKYO', 'LONDON', 'SYDNEY', 'ASIA PAC', 'FORT WORTH', '@FLYTPA', 'SEA', 'ROC', 'INDIA', 'AMSTERDAM', 'EWR', 'PALM SPRINGS', 'HEMISPHERE', 'DURANGO', 'FARMINGTON NEW MEXICO', 'VIRGIN', 'GLASSDOOR', 'JACKSONVILLE', 'MIDWAY-CHICAGO', 'BIRMINGHAM', 'CHANTILLY', 'FRANCE', 'TAMPA', 'WASHINGTON DC', 'AMERICA', 'MIDLAND', 'L.A.', 'SAN ANTONIO', 'OHIO', 'OAKLAND', 'SAN FRAN', 'LASALLE', 'BOS', 'NEW ORLEANS', 'ARUBA', 'SAN FRANSISCO', 'ST. LOUIS', 'ARKANSAS', 'LGBT.CAN', '3130', 'AZ', 'OMAHA', 'TAIWAN', 'BALTIMORE', 'INDIANAPOLIS', 'N231WN', 'COLUMBUS', 'FAIRFAX', 'MEXICO CITY', 'MIDWAY', 'BOISE', 'UTAH', 'FORT LAUDERDALE', 'COLD', 'PROVO', 'PLEASEEEEEE', 'HOU', 'PUERTO VALLARTA', '@BOSTONLOGAN', 'MILWAUKEE', 'ORLANDO', 'BEIJING', 'SAVANNAH', 'IRELAND', 'PHOENIX', '@VINYLVEGAS', 'SAN DIEGO @SOUTHWESTAIR', 'DOMINICAN REPUBLIC', 'JKF', 'HTTP://T.CO/VXN2J36M7V', 'CHARLESTON', 'HTTP://T.CO/KWUEK1UKBC', 'BAHAMAS', 'HTTP://T.CO/IRIXAIFJJX', 'YASSSSS', 'WELLLLLLL', 'ðŸ˜•', 'HTTP://T.CO/PULP4I0W96', 'CUBA', 'PITTSBURGH', 'â˜º', 'CPAP', 'NASSAU', 'PITTS', 'HAITI', 'CALIFORNIA', 'ST LUCIA', 'BQN', 'D.C.', 'HAVANA', 'CARTAGO', 'CLEVELAND', 'HUNTSVILLE', 'AL', 'BRAZIL', 'ICELAND', 'ITALY', 'NAPLES', 'NEW LIVERY', '@BOSTONLOGAN HTTP://T.CO/QEADA92MW6 @JETBLUE', 'WASHINGTON  DC', 'PUERTO RICO', 'JAMAICA', 'WASHINGTON', 'CHARLOTTE', 'CAROLINAS', 'ISRAEL', 'UK', 'QUINTANA ROO', '@PDQUIGLEY', 'DFW', 'KPHL', 'LYNCHBURG', 'ONTARIO', 'LEXINGTON', 'GAINESVILLE', 'USAIRWAYS', 'ARIZONA', 'NC', 'SAN MIGUEL DE ALLENDE', 'BARBADOS', 'AM.SHOULD', 'NUTELLA', 'INTRA-EU', 'WEST PALM BEACH', 'DUBLIN', 'HERNDON', 'VIRGINIA', 'AUH', 'ECUADOR', 'RUSSIA', 'NO', 'OREGON', 'ALASKA', 'ZURICH', 'SAN DIEGO @AMERICANAIR']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6sO71qRJKi-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "afbfec81-5add-4e6a-f6b7-a8de8de7ff66"
      },
      "source": [
        "#next thing we're bringing in is the United Airlines corpus. \n",
        "#If a tweet is negative, append its text to the rough text file and then do NLPre on it\n",
        "import pandas as pd\n",
        "from textblob import TextBlob\n",
        "import spacy\n",
        "from csv import reader\n",
        "from csv import writer\n",
        "\n",
        "dataset = pd.read_csv('/content/data.csv')\n",
        "comments = dataset['comment']\n",
        "polarity = []\n",
        "subjectivity = []\n",
        "for comment in comments:\n",
        "    blob = TextBlob(comment)\n",
        "    polarity.append(blob.sentiment[0])\n",
        "    subjectivity.append(blob.sentiment[1])\n",
        "print(polarity)\n",
        "print(subjectivity)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.9765625, -0.23125, 0.0, 0.0, -0.5, 0.0, 0.0, 0.7, 0.8, 0.4444444444444444, 0.0, 0.0, 0.8, 0.0, 0.033333333333333326, 0.9765625, 0.5, 0.0, 0.0, 0.5, 0.3333333333333333, 0.2, -0.6, -0.30000000000000004, 0.0, 0.0, 0.0, -0.3, 0.1, 0.0, -0.6, 0.0, 0.0, 1.0, 0.0, 0.0, -0.4, -0.5, 0.5, 0.0, 0.4125, 0.625, 0.275, -0.2]\n",
            "[0.6, 0.54375, 1.0, 0.0, 1.0, 0.0, 0.8333333333333334, 0.6000000000000001, 0.75, 0.65, 0.0, 0.0, 0.75, 0.0, 0.55, 0.6, 0.5, 0.0, 0.0, 1.0, 0.5666666666666668, 0.8, 0.7, 0.39999999999999997, 0.0, 0.375, 0.0, 0.3, 0.3, 0.0, 0.9, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6, 0.7666666666666666, 0.6, 0.0, 0.7444444444444445, 0.6, 0.5, 0.8]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlHCShTIqeFV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "outputId": "9dce6b07-7894-4d70-ab7f-fb0dce3f8577"
      },
      "source": [
        "\n",
        "#LOOK INTO THE EMOJIS\n",
        "from emoji import UNICODE_EMOJI\n",
        "from csv import reader\n",
        "from csv import writer\n",
        "from textblob import TextBlob\n",
        "import pandas as pd\n",
        "emojis = []\n",
        "tweets = pd.read_csv('/content/Tweets.csv')\n",
        "text = tweets['text']\n",
        "def has_emoji(sentence): #CHECK IF THERE ARE EMOJIS IN THE SENTENCE\n",
        "  count = 0\n",
        "  for emoji in UNICODE_EMOJI:\n",
        "    if emoji in sentence:\n",
        "      count += 1\n",
        "      if emoji not in emojis:\n",
        "        emojis.append(emoji)\n",
        "    #count += sentence.count(emoji)\n",
        "    #if count > 0 and emoji not in emojis:\n",
        "     # emojis.append(emoji)\n",
        "  if count > 0:\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "#MAIN BODY\n",
        "#EMOJIS INDICATE THAT A TWEET IS AN OPINION, NOT A FACT.\n",
        "index = 0\n",
        "num_removed = 0\n",
        "while index < len(tweets):\n",
        "  if has_emoji(text[index]) == False or 'FLEEK' in text[index].upper():\n",
        "    tweets.drop(index=index)\n",
        "    num_removed +=1\n",
        "  index += 1\n",
        "print(str(num_removed) + \" entries removed\")\n",
        "print(emojis)\n",
        "print(len(emojis))\n",
        "scores = [0.25,0.27,0.2,0.2,0.15,-0.2,-0.25,0.05,0.25,0.05,0.02,0.08,0.3,0.15,0.3,0.2,0.19,0.25,-0.3,-0.25,0.15,-0.1,0.27,-0.08,-0.08,0.1,0.05,0.18,0.06,-0.18,-0.2,0.28,0.25,0.16,0.05,0.03,0.00,0.00,0.00,0.05,0.00,0.00,0.04,-0.16,0.14,-0.19,0.09,0.13,0.13,-0.04,-0.21,0.22,0.06,0.07,0.07,0.09,-0.3,-0.14,-0.08,0.00,-0.3,0.2,0.2,0.11,-0.2,0.2,0.05,0.15,0.12,-0.16,-0.28,-0.25,0.15,-0.16,-0.2,0.03,-0.04,-0.07,0.00,0.16,0.04,-0.16,0.24,0.03,0.1,0.25,0.1,0.25,0.0,0.0,-0.1,-0.15,0.22,-0.06,0.05,0.02,0.06,-0.1,0.1,0.0,0.03,0.1,-0.01,0.17,-0.2,0.06,0.06,-0.1,0.05,-0.15,0.2,0.04,0.01,-0.05,-0.08,0.00,0.00,-0.09,-0.05,0.05,-0.14,0.04,-0.15,0.1,-0.1,0.1,0.01,0.05,0.0,0.1,0.05,-0.1,0.00,0.00]\n",
        "x = list(zip(emojis, scores))\n",
        "print(x)\n",
        "sentiments = []\n",
        "texts = []\n",
        "\"\"\"for tweet in tweets['text']:\n",
        "  score = int(TextBlob(tweet).sentiment[0])\n",
        "  for emoji in x:\n",
        "    if emoji[0] in tweet:\n",
        "      score += emoji[1]\n",
        "    if score > 1:\n",
        "      score = 1\n",
        "    elif score < -1:\n",
        "      score = -1\n",
        "    sentiments.append(score)\n",
        "    texts.append(tweet)\n",
        "results = list(zip(texts,sentiments))\n",
        "print(results)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14182 entries removed\n",
            "['â¤', 'â¤ï¸', 'â˜º', 'â˜ºï¸', 'ðŸ‘', 'ðŸ˜¡', 'ðŸ˜¢', 'âœˆ', 'ðŸ’œ', 'âœˆï¸', 'ðŸ’º', 'ðŸ·', 'ðŸ˜Š', 'ðŸ‘Œ', 'ðŸ˜', 'ðŸ’•', 'ðŸŒž', 'ðŸ˜ƒ', 'ðŸ˜­', 'ðŸ˜©', 'ðŸ˜Ž', 'ðŸ™‰', 'ðŸ˜', 'â„', 'â„ï¸', 'ðŸ‘', 'ðŸ˜‚', 'ðŸ’—', 'ðŸ¸', 'ðŸ˜’', 'ðŸ‘Ž', 'ðŸ˜€', 'ðŸ˜„', 'ðŸ˜˜', 'ðŸ‡ºðŸ‡¸', 'ðŸ‘¸', 'ðŸ‡¸', 'ðŸ‡º', 'ðŸ‡¬ðŸ‡§', 'ðŸŒ', 'ðŸ‡§', 'ðŸ‡¬', 'ðŸŽ€', 'ðŸ˜¥', 'ðŸ˜‰', 'ðŸ˜±', 'âœ¨', 'ðŸŽ‰', 'ðŸ™Œ', 'ðŸ’¤', 'ðŸ˜ž', 'â™¥', 'ðŸ‘‹', 'âœŒ', 'âœŒï¸', 'ðŸ™', 'ðŸ‘¿', 'ðŸ˜”', 'ðŸ™…', 'ðŸ†–', 'ðŸ’©', 'âœ”ï¸', 'âœ”', 'ðŸŒ´', 'âŒ', 'âœ…', 'ðŸ‘ ', 'ðŸ˜œ', 'ðŸ˜»', 'ðŸ˜•', 'ðŸ˜ˆ', 'ðŸ˜¤', 'ðŸ’ª', 'ðŸ˜«', 'ðŸ’”', 'ðŸ˜ª', 'ðŸ˜£', 'ðŸ˜¬', 'ðŸ’', 'ðŸ˜‹', 'ðŸ˜', 'ðŸ˜–', 'ðŸŒŸ', 'ðŸ“±', 'ðŸ»', 'ðŸ’–', 'ðŸ˜…', 'ðŸ’', 'â†”', 'â†”ï¸', 'ðŸš«', 'ðŸ˜·', 'â­', 'â—', 'ðŸŽµ', 'ðŸ´', 'â™¥ï¸', 'ðŸ˜†', 'ðŸ˜‘', 'ðŸ©', 'â¤´', 'â˜€', 'â˜€ï¸', 'ðŸ‘Š', 'ðŸ’¯', 'ðŸ˜ ', 'â˜•', 'ðŸ“²', 'ðŸ‘º', 'ðŸ™ˆ', 'ðŸ’˜', 'ðŸ’™', 'ðŸ‘‰', 'ðŸšª', 'ðŸ˜³', 'ðŸ˜µ', 'ðŸš¶', 'ðŸ”µ', 'ðŸ˜', 'ðŸ‘€', 'ðŸ…', 'ðŸ†˜', 'â›„', 'ðŸ˜“', 'ðŸŽ²', 'âŒš', 'ðŸ³', 'â¤µ', 'ðŸ˜®', 'ðŸ˜²', 'ðŸ˜¦', 'âž¡', 'âž¡ï¸']\n",
            "133\n",
            "[('â¤', 0.25), ('â¤ï¸', 0.27), ('â˜º', 0.2), ('â˜ºï¸', 0.2), ('ðŸ‘', 0.15), ('ðŸ˜¡', -0.2), ('ðŸ˜¢', -0.25), ('âœˆ', 0.05), ('ðŸ’œ', 0.25), ('âœˆï¸', 0.05), ('ðŸ’º', 0.02), ('ðŸ·', 0.08), ('ðŸ˜Š', 0.3), ('ðŸ‘Œ', 0.15), ('ðŸ˜', 0.3), ('ðŸ’•', 0.2), ('ðŸŒž', 0.19), ('ðŸ˜ƒ', 0.25), ('ðŸ˜­', -0.3), ('ðŸ˜©', -0.25), ('ðŸ˜Ž', 0.15), ('ðŸ™‰', -0.1), ('ðŸ˜', 0.27), ('â„', -0.08), ('â„ï¸', -0.08), ('ðŸ‘', 0.1), ('ðŸ˜‚', 0.05), ('ðŸ’—', 0.18), ('ðŸ¸', 0.06), ('ðŸ˜’', -0.18), ('ðŸ‘Ž', -0.2), ('ðŸ˜€', 0.28), ('ðŸ˜„', 0.25), ('ðŸ˜˜', 0.16), ('ðŸ‡ºðŸ‡¸', 0.05), ('ðŸ‘¸', 0.03), ('ðŸ‡¸', 0.0), ('ðŸ‡º', 0.0), ('ðŸ‡¬ðŸ‡§', 0.0), ('ðŸŒ', 0.05), ('ðŸ‡§', 0.0), ('ðŸ‡¬', 0.0), ('ðŸŽ€', 0.04), ('ðŸ˜¥', -0.16), ('ðŸ˜‰', 0.14), ('ðŸ˜±', -0.19), ('âœ¨', 0.09), ('ðŸŽ‰', 0.13), ('ðŸ™Œ', 0.13), ('ðŸ’¤', -0.04), ('ðŸ˜ž', -0.21), ('â™¥', 0.22), ('ðŸ‘‹', 0.06), ('âœŒ', 0.07), ('âœŒï¸', 0.07), ('ðŸ™', 0.09), ('ðŸ‘¿', -0.3), ('ðŸ˜”', -0.14), ('ðŸ™…', -0.08), ('ðŸ†–', 0.0), ('ðŸ’©', -0.3), ('âœ”ï¸', 0.2), ('âœ”', 0.2), ('ðŸŒ´', 0.11), ('âŒ', -0.2), ('âœ…', 0.2), ('ðŸ‘ ', 0.05), ('ðŸ˜œ', 0.15), ('ðŸ˜»', 0.12), ('ðŸ˜•', -0.16), ('ðŸ˜ˆ', -0.28), ('ðŸ˜¤', -0.25), ('ðŸ’ª', 0.15), ('ðŸ˜«', -0.16), ('ðŸ’”', -0.2), ('ðŸ˜ª', 0.03), ('ðŸ˜£', -0.04), ('ðŸ˜¬', -0.07), ('ðŸ’', 0.0), ('ðŸ˜‹', 0.16), ('ðŸ˜', 0.04), ('ðŸ˜–', -0.16), ('ðŸŒŸ', 0.24), ('ðŸ“±', 0.03), ('ðŸ»', 0.1), ('ðŸ’–', 0.25), ('ðŸ˜…', 0.1), ('ðŸ’', 0.25), ('â†”', 0.0), ('â†”ï¸', 0.0), ('ðŸš«', -0.1), ('ðŸ˜·', -0.15), ('â­', 0.22), ('â—', -0.06), ('ðŸŽµ', 0.05), ('ðŸ´', 0.02), ('â™¥ï¸', 0.06), ('ðŸ˜†', -0.1), ('ðŸ˜‘', 0.1), ('ðŸ©', 0.0), ('â¤´', 0.03), ('â˜€', 0.1), ('â˜€ï¸', -0.01), ('ðŸ‘Š', 0.17), ('ðŸ’¯', -0.2), ('ðŸ˜ ', 0.06), ('â˜•', 0.06), ('ðŸ“²', -0.1), ('ðŸ‘º', 0.05), ('ðŸ™ˆ', -0.15), ('ðŸ’˜', 0.2), ('ðŸ’™', 0.04), ('ðŸ‘‰', 0.01), ('ðŸšª', -0.05), ('ðŸ˜³', -0.08), ('ðŸ˜µ', 0.0), ('ðŸš¶', 0.0), ('ðŸ”µ', -0.09), ('ðŸ˜', -0.05), ('ðŸ‘€', 0.05), ('ðŸ…', -0.14), ('ðŸ†˜', 0.04), ('â›„', -0.15), ('ðŸ˜“', 0.1), ('ðŸŽ²', -0.1), ('âŒš', 0.1), ('ðŸ³', 0.01), ('â¤µ', 0.05), ('ðŸ˜®', 0.0), ('ðŸ˜²', 0.1), ('ðŸ˜¦', 0.05), ('âž¡', -0.1), ('âž¡ï¸', 0.0)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"for tweet in tweets['text']:\\n  score = int(TextBlob(tweet).sentiment[0])\\n  for emoji in x:\\n    if emoji[0] in tweet:\\n      score += emoji[1]\\n    if score > 1:\\n      score = 1\\n    elif score < -1:\\n      score = -1\\n    sentiments.append(score)\\n    texts.append(tweet)\\nresults = list(zip(texts,sentiments))\\nprint(results)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0egTfWQdT6dZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "400b555d-28c1-41c9-8bb9-c458553b6f89"
      },
      "source": [
        "emojis = ['â¤', 'â¤ï¸', 'â˜º', 'â˜ºï¸', 'ðŸ‘', 'ðŸ˜¡', 'ðŸ˜¢', 'âœˆ', 'ðŸ’œ', 'âœˆï¸', 'ðŸ’º', 'ðŸ·', 'ðŸ˜Š', 'ðŸ‘Œ', 'ðŸ˜', 'ðŸ’•', 'ðŸŒž', 'ðŸ˜ƒ', 'ðŸ˜­', 'ðŸ˜©', 'ðŸ˜Ž', 'ðŸ™‰', 'ðŸ˜', 'â„', 'â„ï¸', 'ðŸ‘', 'ðŸ˜‚', 'ðŸ’—', 'ðŸ¸', 'ðŸ˜’', 'ðŸ‘Ž', 'ðŸ˜€', 'ðŸ˜„', 'ðŸ˜˜', 'ðŸ‡ºðŸ‡¸', 'ðŸ‘¸', 'ðŸ‡¸', 'ðŸ‡º', 'ðŸ‡¬ðŸ‡§', 'ðŸŒ', 'ðŸ‡§', 'ðŸ‡¬', 'ðŸŽ€', 'ðŸ˜¥', 'ðŸ˜‰', 'ðŸ˜±', 'âœ¨', 'ðŸŽ‰', 'ðŸ™Œ', 'ðŸ’¤', 'ðŸ˜ž', 'â™¥', 'ðŸ‘‹', 'âœŒ', 'âœŒï¸', 'ðŸ™', 'ðŸ‘¿', 'ðŸ˜”', 'ðŸ™…', 'ðŸ†–', 'ðŸ’©', 'âœ”ï¸', 'âœ”', 'ðŸŒ´', 'âŒ', 'âœ…', 'ðŸ‘ ', 'ðŸ˜œ', 'ðŸ˜»', 'ðŸ˜•', 'ðŸ˜ˆ', 'ðŸ˜¤', 'ðŸ’ª', 'ðŸ˜«', 'ðŸ’”', 'ðŸ˜ª', 'ðŸ˜£', 'ðŸ˜¬', 'ðŸ’', 'ðŸ˜‹', 'ðŸ˜', 'ðŸ˜–', 'ðŸŒŸ', 'ðŸ“±', 'ðŸ»', 'ðŸ’–', 'ðŸ˜…', 'ðŸ’', 'â†”', 'â†”ï¸', 'ðŸš«', 'ðŸ˜·', 'â­', 'â—', 'ðŸŽµ', 'ðŸ´', 'â™¥ï¸', 'ðŸ˜†', 'ðŸ˜‘', 'ðŸ©', 'â¤´', 'â˜€', 'â˜€ï¸', 'ðŸ‘Š', 'ðŸ’¯', 'ðŸ˜ ', 'â˜•', 'ðŸ“²', 'ðŸ‘º', 'ðŸ™ˆ', 'ðŸ’˜', 'ðŸ’™', 'ðŸ‘‰', 'ðŸšª', 'ðŸ˜³', 'ðŸ˜µ', 'ðŸš¶', 'ðŸ”µ', 'ðŸ˜', 'ðŸ‘€', 'ðŸ…', 'ðŸ†˜', 'â›„', 'ðŸ˜“', 'ðŸŽ²', 'âŒš', 'ðŸ³', 'â¤µ', 'ðŸ˜®', 'ðŸ˜²', 'ðŸ˜¦', 'âž¡', 'âž¡ï¸']\n",
        "scores = [0.25,0.27,0.2,0.2,0.15,-0.2,-0.25,0.05,0.25,0.05,0.02,0.08,0.3,0.15,0.3,0.2,0.19,0.25,-0.3,-0.25,0.15,-0.1,0.27,-0.08,-0.08,0.1,0.05,0.18,0.06,-0.18,-0.2,0.28,0.25,0.16,0.05,0.03,0.00,0.00,0.00,0.05,0.00,0.00,0.04,-0.16,0.14,-0.19,0.09,0.13,0.13,-0.04,-0.21,0.22,0.06,0.07,0.07,0.09,-0.3,-0.14,-0.08,0.00,-0.3,0.2,0.2,0.11,-0.2,0.2,0.05,0.15,0.12,-0.16,-0.28,-0.25,0.15,-0.16,-0.2,0.03,-0.04,-0.07,0.00,0.16,0.04,-0.16,0.24,0.03,0.1,0.25,0.1,0.25,0.0,0.0,-0.1,-0.15,0.22,-0.06,0.05,0.02,0.06,-0.1,0.1,0.0,0.03,0.1,-0.01,0.17,-0.2,0.06,0.06,-0.1,0.05,-0.15,0.2,0.04,0.01,-0.05,-0.08,0.00,0.00,-0.09,-0.05,0.05,-0.14,0.04,-0.15,0.1,-0.1,0.1,0.01,0.05,0.0,0.1,0.05,-0.1,0.00,0.00]\n",
        "combined = list(zip(emojis, scores))\n",
        "tweets = pd.read_csv('/content/Tweets.csv')\n",
        "text = tweets['text']\n",
        "for tweet in tweets['text']:\n",
        "  score = int(TextBlob(tweet).sentiment[0])\n",
        "  for emoji in combined:\n",
        "    if emoji[0] in tweet:\n",
        "      score += emoji[1]\n",
        "    if score > 1:\n",
        "      score = 1\n",
        "    elif score < -1:\n",
        "      score = -1\n",
        "    sentiments.append(score)\n",
        "    texts.append(tweet)\n",
        "results = list(zip(texts,sentiments))\n",
        "print(results)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hf9NNjGAbls-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "658f06e5-faa3-4d41-c3df-76694c7843f5"
      },
      "source": [
        "with open('fulltext.txt','r') as file:\n",
        "  collection = file.read()\n",
        "  hashtags = {tag.strip(\"#\") for tag in collection.split() if tag.startswith(\"#\")}\n",
        "  print(hashtags)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'', 'Luv', 'mileagerun', 'EWR.', 'AvGeek', 'InDenial', 'JetBlueBruins', 'escape', 'incompetent', 'FareCompare', 'cause', 'Nantucket', 'lovetotravel', 'confused', '80sweresomuchfun', 'epicfail', 'flying', 'charity', 'OAK', 'IAH', 'warmweather', 'thestarter??ðŸ˜', '.', 'united13', 'honesty', 'feelsgood', 'dayjustgotWORSE!', 'save', 'ggqzqd', 'no800number', 'OscarsCountdown', '59', 'MedusaFridays', 'rewards', 'eventhoughits2degreesathome', 'mint', '?', 'sunscreen', 'deice', 'thefutureisweird', 'DoBetter', 'Dallas', '12thMan', 'hurt.', 'wewillsee', 'B767-300ER', 'SouthBendINWhere', 'loyal', 'pilots', '2522', 'ThankYou', 'filmcrew', '1715', 'AmericanAirlines', 'biztravel', 'UTDallas', 'Octavia', 'ftw!', 'innovacion', 'Portland', '3900', 'SoundOfMusic', 'ControllableIrregularity', 'Apple', 'ERJ145', '150219-000114', 'CompanionPasses', 'prettyplease', 'NewOrleans', 'TSAPreCheck', 'maybeijustlost', 'Freberg15', 'DestinationDragon', 'imaginedragons', 'fb', 'jailbreak', 'shoutout', 'payments', 'sunkist', 'hotlanta', 'Anaphylaxis,will', \"Apple's\", 'passengers', 'JVMChat', '1870', 'snowwillnevermelt', 'change', 'flyingitforward', 'usairwaysfail', 'RaganDisney', 'logic', 'TV4U', 'love', 'OSCARS2105', 'chaching', 'brandmance', 'Travel', 'ijustwanttosleep', 'Business', 'feeltheheat', 'AA65', 'andchexmix', 'FL', 'sm', 'IWouldDoAnythingForLove', 'SouthwestRally', 'areyounew?', 'cnn', 'FlyingitForward', 'bos', 'ilovejetblue', 'loyalty', 'February.â€¦', 'tinderchamp', 'negativedegrees', 'RedCarpet', 'travel', 'SWA', 'visa', 'Caucasity', 'Milan', 'consumermarketing', 'Aviation', 'busads', 'happytohelp', 'AvalonHollywood', 'deaffriendly?', 'NewAmericanStinks', 'Nashville', 'nashville?', 'MiddleEast', 'MHTforlife', 'GonnaBeALongNight', 'burningman', 'Heathrow', 'destinationdragons.', 'playsoon', 'Philly', 'DallasAirport.I', 'keepem2', 'DCA', 'MayweatherPacquiao', 'letitgo', 'redcarpet', 'LuvSWA', 'notsurprising', 'TinderTips', 'amypoehler.', 'sportsbiz', 'tribute', 'iPodTouch,', 'iPad.', 'Delta', 'maintenance', 'shouldhaveflowndelta', 'parentsonboard', 'BrandMance', 'ScienceBehindTheExperience', 'planestrainsandautomobiles', 'firststari', 'rude', 'LikeAGirl', 'Cheesy', 'MileagePlus', '1!', 'jetblue', '611?', 'Oakland', 'mdw2mci', 'flight353', 'flightlanding', 'FlyPBI', 'sendambien', 'STL', 'statusmatch', 'nerdbird', 'chairman', 'avgeek', 'sunrise', 'foh', 'SanDiego.', 'chrome', 'Columbus', 'BestInClassSocial', 'airlines', 'Brand', 'LoveSongFriday', 'FlyingItForward', 'happy4them', 'OrangeCounty', 'winwin', 'wish', 'NYC', 'Vegas', 'okcdirects', 'makestoomuchsense', 'vegetarianproblems', 'DaytonaBeach,', 'USA', 'noworstairline', 'disappointed', 'B737-700', 'ITproblems', 'Bandie', 'concerned', 'Flight', 'SouthwestAir', '2daysLate', 'heart', 'agcommunity', 'Lufthansa', '3ticketsforJax!', 'grandcayman', 'Argentina', 'Airlines', 'ChrisHasMadeUsBLUSH', 'SFO', 'TheMenOfBusiness', 'PITT....', 'marketing', '336', '3729...but', 'rockstar', 'bestflightever', '1051', 'slaycancerwithdragons', 'companionpass', 'WinterWeather', 'Oahu', 'BNA??', 'aircargo', 'ModeloDeNegocio', 'Newark', 'SilverStatus', 'gaincustomers', 'bliss', 'frequentflyer', 'A319', 'CMFat35000feet', 'weather', 'vrm', 'SharkTank', '787', 'SWADiversity', 'flights,', '3thparty', 'finalstretch', 'TrueBlue', 'SanFrancisco', 'LUVthem', 'delayedovernight', 'higherandhigher', 'travelers', 'ORD', 'flyingRetro', 'MD80', 'A320', 'oak', 'thingsishouldknow', 'BusHug', 'Cancun', 'SMM', 'MoodlitMonday', '1761', 'Oakland,', 'please', 'Real', '.800#called&amp;it', 'whyfly', 'PointsMe', 'snowbama', 'Sexy', 'DestinationDragons?!', 'Orlando', 'Canada?', 'AIF2015', 'makeitright', 'freyasfund', 'ourprincess', 'Washington', 'dullestostatecollege', 'AA', 'buffalo', 'ItWasMintToBe', 'sofly', 'Passbook', 'BestCrew', 'Oscars2015', 'SaveTheDiagonals', 'LaGuardia', 'custserv', 'thinkbus', 'TheRoFo', 'JSOM', 'UpWhereWeBelong', 'etailwest', 'WheelsUp', 'impact', 'NoPlaceLikeHome', 'BNASnow', 'planes', 'letsgo!', 'veryimportantproject', 'magazine', 'workhard', 'stpatricksfoundation', 'Disney', 'AfterAll', 'TheMagicalStranger', 'mobileboarding', 'wager', 'nerdbird?', 'goingtovegas', 'LGA', 'AnyoneThere', 'AmericanAirlines\"', 'merger', 'TCMParty', 'homeandreadyfornexttrip', 'westpalmbeachbound', 'ifeeldumb', 'Ohio', 'help', '22...\"', 'Lucky', 'RAG', 'Midway', 'budget', 'fleek', 'B737-900', 'JFK', 'airline', 'gross', 'stepup', 'dc', 'DestinationDragons', '3768', 'okcprofessionals', 'MCI', 'TheTakeover,', 'Austin', '767', 'runningonthreehoursofsleep', 'scam?', 'nbc', 'Explain', 'IDontWannaLiveWithoutYourLove', '4372', 'ANAmarketers', 'VXSafetyDanceâ€', 'rotary', 'Hug', '3854', 'SOBEWFF', 'BOS', 'Frankfurt', 'healthbenefitsofplants', 'oaaret', 'FlyItForward', 'OKC', 'sfo', 'MDW,â€¦', 'FtLauderdale', 'BusinessModel', 'Books', 'HelpMePlease', 'deltanews', 'B767-400ER', 'whyyounoloveme', 'Southwest', '3526665682.', 'glassslipperchallenge', 'Emirates', 'B777-200ER', 'PoorForm', 'ATL', 'CE3K', 'NFTYConvention', 'countingdown', 'SJC', 'OHare', 'flights', 'CLT', '31DaysOfOscar', 'resend?', 'peanutsandtoons', 'winter...', 'Evansville', 'media', 'HappyFlight', 'miami', 'BrandLoveAffair', 'safetyconcerns', 'quiet', 'Brrr', 'AmericanView', 'ItsCold', 'FlyFi', '561?', 'Cuba,', 'IChangedYourDiaper', 'ripskymall', 'virginamerica', 'promotion', 'cmh', 'Florida', 'innovation', 'IfThe80sNeverStopped,', 'DFW', 'LAS2SFO', 'forevercold', '1', 'Pilot', 'unitedsucks', 'OnFleek', 'Furious', 'gracias', 'Hawaii', 'dog', 'ApplePay', 'Boston', 'CarrieUnderwood', 'USAirways', 'FloridaVacation', 'goodnight', 'aviation', 'flyitforward', 'Issues', 'SheRocks', 'travelhelp', 'CheapFlights', 'E190', 'Zurich', 'VeryLoyalCustomer', 'Another', 'Malpensa', 'customerappreciation', 'WhatFrozenPipes', 'IsItSummerYet', 'business', 'swag', '87yearsLate', 'execplat', 'ClearVision', 'bestplanesever', 'swaculture', 'iPhone', 'Monterey', 'gardening', 'mileageplus', 'bestemployees', 'DAL', 'BWI', 'Passbook.', 'UnitedAirlines', 'FtMyers', 'Chicago', 'LAX', 'MardiGras', 'GolfUnited', ':', '555PHLtoSLC', 'milehighselfieclub'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TnZ1Q-3Z1aJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#FIND OUT WHAT THE #'s that are used in the dataset\n",
        "tags=\"Hey guys! #stackoverflow really #rocks #rocks #announcement\"\n",
        "tag.strip(\"#\") for tag in tags.split() if tag.startswith(\"#\")}\n",
        "set(['announcement', 'rocks', 'stackoverflow'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abOE18BGWQOT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from spacy.lang.en import English\n",
        "from textblob import TextBlob\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# Load English tokenizer, tagger, parser, NER and word vectors\n",
        "nlp = English()\n",
        "path = 'C://Users/rjdpo/Downloads/Tweets.csv'\n",
        "tweets = pd.read_csv(path)\n",
        "from spacy.lang.en import English\n",
        "\n",
        "# Load English tokenizer, tagger, parser, NER and word vectors\n",
        "nlp = English()\n",
        "\n",
        "text = tweets['text'][1]\n",
        "#  \"nlp\" Object is used to create documents with linguistic annotations.\n",
        "my_doc = nlp(text)\n",
        "my_blob = TextBlob(text)\n",
        "print(my_blob.tags)\n",
        "# Create list of word tokens\n",
        "token_list = []\n",
        "for token in my_doc:\n",
        "    token_list.append(token.text)\n",
        "\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "# Create list of word tokens after removing stopwords\n",
        "filtered_sentence =[] \n",
        "\n",
        "for word in token_list:\n",
        "    lexeme = nlp.vocab[word]\n",
        "    if lexeme.is_stop == False:\n",
        "        filtered_sentence.append(word) \n",
        "print(token_list)\n",
        "print(filtered_sentence)\n",
        "\n",
        "#SPLITTING ALGORITHM\n",
        "removals = ['[',']']\n",
        "for coords in tweets['tweet_coord']:\n",
        "    if coords != '':\n",
        "        str(coords).replace('[','')\n",
        "        str(coords).replace(']','')\n",
        "        tweets['latitude'] = str(coords).split(',')[0]\n",
        "        tweets['longitude'] = str(coords).split(',')[-1]\n",
        "#PLOT SENTIMENTSCORES\n",
        "total = 0;\n",
        "count = 0;\n",
        "airlines = ['@SOUTHWESTAIR','@VIRGINAMERICA','@UNITED','@JETBLUE','@USAIRWAYS','@AMERICANAIR']\n",
        "keyword = ' THANKS'\n",
        "for name in airlines:\n",
        "    for tweet in tweets['text']:\n",
        "        if name in str(tweet).upper() and keyword in str(tweet).upper():\n",
        "            total = total +1\n",
        "            blob = TextBlob(tweet)\n",
        "            #print(str(tweet) + ' ' + str(blob.sentiment[0]))\n",
        "            if blob.sentiment[0] < 0:\n",
        "                count = count + 1 \n",
        "    print(name + ': ' + str(count) + ' out of ' + str(total))\n",
        "    total = 0\n",
        "    count = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugzHgySZBqmp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#FINAL BLOCK OF CODE\n",
        "#ADD HEADINGS TO AIRLINE INDIVIDUAL DATASETS\n",
        "import csv\n",
        "with open('/content/delta.csv',newline='') as f:\n",
        "    r = csv.reader(f)\n",
        "    data = [line for line in r]\n",
        "with open('/content/delta.csv','w',newline='') as f:\n",
        "    w = csv.writer(f)\n",
        "    w.writerow(['tweet_ID','sentiment', 'sentiment_confidence', 'negative_reason','negative_reason_confidence','airline','airline_sentiment_gold','name','negative_reason_gold','retweets','text','coordinates','creation_time','location','timezone'])\n",
        "    w.writerows(data)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgDnNLqfdHoo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#SOCIAL MEDIA CAMPAIGN FOR DELTA MAKES UP A LARGE PORTION OF ITS SET OF TWEETS, SKEWING ITS RATINGS.\n",
        "#CODE FOR REMOVING 'FLEEK' TWEETS\n",
        "import pandas as pd\n",
        "import re\n",
        "delta = pd.read_csv('/content/delta.csv')\n",
        "def iterate():\n",
        "  for index,row in delta.iterrows():\n",
        "    print(index, row)\n",
        "\n",
        "\n",
        "def has_fleek(sentence):\n",
        "  return 'FLEEK' in sentence.upper()\n",
        "\n",
        "def get_fleeks():\n",
        "  return [text for text in delta['text'] if has_fleek(text)]\n",
        "\n",
        "def destroy_fleeks(tweetset=pd.DataFrame()) -> pd.DataFrame: \n",
        "    return tweetset.loc[tweetset['text'].str.contains('fleek')]\n",
        "\n",
        "def is_long_tweet(text):\n",
        "  return len(text) > 10\n",
        "\n",
        "for i, row in enumerate(delta['text'], start=0):\n",
        "  if is_long_tweet(row):\n",
        "    print(row)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}